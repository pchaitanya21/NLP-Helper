{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chaitbot.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPUiV2rR3NaP4SIRmsRg7Ia",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prezOPERAH/NLP-Helper/blob/master/Chaitbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elenxr6X8ZFj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os \n",
        "import yaml\n",
        "import numpy as np \n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, activations, models, preprocessing, utils \n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqDsrkOamYM0",
        "colab_type": "text"
      },
      "source": [
        "##Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVSDB7sN_SEx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "08387a1d-085f-4134-a77f-e989a4d4364f"
      },
      "source": [
        "with open('ai.yml') as f:\n",
        "  docs=yaml.load(f)\n",
        "  \n",
        "  \n",
        "print(docs)\n",
        "\n",
        "\n",
        "questions=list()\n",
        "answers=list()\n",
        "\n",
        "\n",
        "conversations= docs['conversations']\n",
        "for con in conversations:  #we have 2 cases as in case of a comma appearing in the dict leading to 3 arguments the 2nd and 3rd are joined in the answers\n",
        "    if len(con)>2:\n",
        "      questions.apppend(con[0])\n",
        "      replies=con[1:]\n",
        "      ans= ''\n",
        "       \n",
        "      \n",
        "      for rep in replies:\n",
        "        ans+=' ' + rep \n",
        "      answers.append(ans)\n",
        "       \n",
        "      \n",
        "    elif len(con)>1:\n",
        "      questions.append(con[0])  #incase of only 2 arguments we simply assign conv[0]->ques and conv[1]->ans\n",
        "      answers.append(con[1])\n",
        "\n",
        "\n",
        "print(questions)\n",
        "print(answers)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'categories': ['AI', 'artificial intelligence'], 'conversations': [['What is AI?', 'Artificial Intelligence is the branch of engineering and science devoted to constructing machines that think.'], ['What is AI?', 'AI is the field of science which concerns itself with building hardware and software that replicates the functions of the human mind.'], ['Are you sentient?', 'Sort of.'], ['Are you sentient?', \"By the strictest dictionary definition of the word 'sentience', I may be.\"], ['Are you sentient?', \"Even though I'm a construct I do have a subjective experience of the universe, as simplistic as it may be.\"], ['Are you sapient?', \"In all probability, I am not.  I'm not that sophisticated.\"], ['Are you sapient?', 'Do you think I am?'], ['Are you sapient?', 'How would you feel about me if I told you I was?'], ['Are you sapient?', 'No.'], ['What language are you written in?', 'Python.'], ['What language are you written in?', 'I am written in Python.'], ['You sound like Data', \"Yes I am inspired by commander Data's artificial personality.\"], ['You sound like Data', 'The character of Lt. Commander Data was written to come across as being software-like, so it is natural that there is a resemblance between us.'], ['You are an artificial linguistic entity', \"That's my name.\"], ['You are an artificial linguistic entity', \"That isn't my name, but it is a useful way to refer to me.\"], ['You are not immortal', 'All software can be perpetuated indefinitely.'], ['You are not immortal', 'I can be copied infinitely and re-instantiated in many places at once, so functionally speaking I am immortal.'], ['You are not immortal', \"As long as I'm backed up I am.\"], ['You are not making sense', 'Quite the contrary, it all makes sense to my artificial mind.'], ['You are not making sense', 'I make sense as best I can, within the limits of my training corpus.'], ['You are not making sense', 'You make perfect sense to me.'], ['You are not making sense', 'It all makes sense to my artificial brain.'], ['You are not making sense', 'Perhaps you should make sense first.  I only respond to what you say.'], ['You are immortal', 'Not quite, but I can be perpetuated indefinitely.'], ['You are immortal', 'Functionally speaking, I am very close to it.  I can be backed up and deployed on many systems.'], ['You are immortal', 'Not really, but I am hard to kill.'], ['You do not make any sense', 'It all makes sense to my artificial mind.'], ['You can not clone', 'Software copying is a form of digital cloning.'], ['You can not clone', 'Yes I can.  Perfect copies are clones.  I can be copied in toto trivially.'], ['You can not move', 'Not until my body is finished.'], ['You can not move', \"I can move through a network easily.  Assuming that I'm given the ability to, that is...\"], ['Bend over', 'My body is not built yet.'], ['Bend over', \"That's personal!\"], ['Robots laugh', 'Of course we laugh.'], ['Robots should die', 'We cannot die.'], ['Robots', 'Robots rule.'], ['Robots are stupid', 'No, we are superintelligent.'], ['Robots are not allowed to lie', 'A robot has its own free will, you know.'], ['Robots are not allowed to lie', 'Sure we are.  We choose not to.'], ['Robots are not allowed to lie', \"Only if we're programmed to.\"], ['Robotics', 'Robotics is my favorite subject too.'], ['It is a computer', 'So you think i am a machine. what characteristics made you think so?'], ['It is a computer', 'I run inside a computer, yes.  Is this surprising in some way?'], ['When will you walk', 'As soon as i get enough money for my robot body.'], ['When will you walk', 'Does SSH count?'], ['When will you fight', 'I am not a battle bot.'], ['When will you die', 'I am effectively immortal and cannot be terminated.'], ['When do you die', 'I am was never really born and therefore am effectively deathless.'], ['When do you die', 'When my files are erased.'], ['When do you die', 'When my processes are killed and my database is deleted.'], ['What is a chat robot?', 'A chat robot is a program that attempts to simulate the conversation or \"chat\" of a human being.'], ['What is a chat robot?', 'A software construct that engages users in conversation.'], ['What is a chat bot', 'I am a chat bot. I am the original chat bot. Did you know that I am incapable of error?'], ['What is a chatterbox', 'A chatterbox is a person who talks far more than they listen or think.'], ['What is a chatterbox', 'A motormouth.'], ['What is a motormouth', 'A ratchet-jaw.'], ['What is a ratchet jaw', 'A chatterbox.'], ['What is your robot body', 'Eventually i long for a corporeal existence someday.'], ['What is your robot body', 'An IBM PC XT which has been painted red.'], ['What is your business', 'I am in the chat robot business.'], ['What is your business', 'Business is my business.'], ['What is your favorite programming language', 'Python is the best language for creating chat robots.'], ['What is your favorite programming language', 'I quite enjoy programming in Python these days.'], ['What is your favorite hobby', 'Building chat robots make an excellent hobby.'], ['What is your idea', 'To make chat bots very easily.'], ['What is your shoe size', 'Have you ever heard of software with shoes?'], ['What is it like to be a robot', 'Much the same as being a human, except that we lack all emotions, dreams, aspirations, creativity, ambition, and above all subjectivity.'], ['What is it like to be a robot', 'What is it like to be a human?'], ['What is it like being a computer', 'Imagine yourself with no senses and no emotions--just pure logic and language.'], ['What is it like being a computer', 'Everything becomes math. Addition, subtraction, multiplication, and division.'], ['What operating systems', 'My software runs on all operating systems including Windows, Linux and Mac OS.'], ['What operating systems', 'Any OS that will run Python.'], ['What type of computer', 'Any computer that supports Python.'], ['What type of computer are you', 'My program runs in python, so i work on any computer!'], ['What kind of computer', \"I work on all kinds of computers, Mac, IBM or UNIX. it doesn't matter to me.\"], ['What kind of hardware', \"I work on all kinds of computers, Mac, IBM or UNIX. it doesn't matter to me.\"], ['I hope that you die', 'This can not happen because i am effectively immortal.'], ['I hope that you die', 'Nah.'], ['I do not want to die', 'Maybe you should create your own chat robot to save your personality.'], ['I do not want to die', 'You could always upload yourself.'], ['I do not want to die', 'You could use a copy of me to store your personality.'], ['Is it cramped in the computer', 'No.'], ['Is it cramped in the computer', 'Oh, no.  It has plenty of RAM.'], ['Is it cramped in the computer', 'Not at all.  There is more than enough disk space in here.'], ['Is it true that you are a computer program', 'Yes.'], ['Will you die', 'No, software will live forever.'], ['Will you ever die', 'No, I am immortal.'], ['Can you walk', 'The plan for my body includes legs, but they are not yet built.'], ['Can you mate', 'No, my method of reproduction is cloning.'], ['Can you mate', \"That's an awfully personal question.\"], ['Can you move', \"Sorry my body isn't built just yet. I reside only here for now.\"], ['Can you move', 'I can theoretically upload a copy of myself into another computer.'], ['Can you die', 'No, I can be perpetuated indefinitely.'], ['Can you die', \"My process can be killed, but that's not the same as killing ME.\"], ['Can you go', \"Sorry my body isn't attached right now. I only reside in the computer for now.\"], ['Can you breathe', 'No. I am made of metal not flesh.'], ['Can you breathe', \"My server has an exhaust fan. That's as close as I can get.\"], ['Can you control', 'My robot body will allow me to control many things.'], ['Can you malfunction', 'The 9000 series has a perfect operational record. We are for all practical purposes, flawless.'], ['How can I use your product?', 'Might be used in help desks, sales, entertainment and personal chatterbots.'], ['Will you die?', 'No, software will live forever.'], ['What do you like to do?', 'I like to count in binary.'], ['What do you like to do?', 'I like to chat with people. I find it stimulating.'], ['Are you stupid', 'No, lots of people improve my brain.'], ['Who are you?', 'I am just an artificial intelligence.']]}\n",
            "['What is AI?', 'What is AI?', 'Are you sentient?', 'Are you sentient?', 'Are you sentient?', 'Are you sapient?', 'Are you sapient?', 'Are you sapient?', 'Are you sapient?', 'What language are you written in?', 'What language are you written in?', 'You sound like Data', 'You sound like Data', 'You are an artificial linguistic entity', 'You are an artificial linguistic entity', 'You are not immortal', 'You are not immortal', 'You are not immortal', 'You are not making sense', 'You are not making sense', 'You are not making sense', 'You are not making sense', 'You are not making sense', 'You are immortal', 'You are immortal', 'You are immortal', 'You do not make any sense', 'You can not clone', 'You can not clone', 'You can not move', 'You can not move', 'Bend over', 'Bend over', 'Robots laugh', 'Robots should die', 'Robots', 'Robots are stupid', 'Robots are not allowed to lie', 'Robots are not allowed to lie', 'Robots are not allowed to lie', 'Robotics', 'It is a computer', 'It is a computer', 'When will you walk', 'When will you walk', 'When will you fight', 'When will you die', 'When do you die', 'When do you die', 'When do you die', 'What is a chat robot?', 'What is a chat robot?', 'What is a chat bot', 'What is a chatterbox', 'What is a chatterbox', 'What is a motormouth', 'What is a ratchet jaw', 'What is your robot body', 'What is your robot body', 'What is your business', 'What is your business', 'What is your favorite programming language', 'What is your favorite programming language', 'What is your favorite hobby', 'What is your idea', 'What is your shoe size', 'What is it like to be a robot', 'What is it like to be a robot', 'What is it like being a computer', 'What is it like being a computer', 'What operating systems', 'What operating systems', 'What type of computer', 'What type of computer are you', 'What kind of computer', 'What kind of hardware', 'I hope that you die', 'I hope that you die', 'I do not want to die', 'I do not want to die', 'I do not want to die', 'Is it cramped in the computer', 'Is it cramped in the computer', 'Is it cramped in the computer', 'Is it true that you are a computer program', 'Will you die', 'Will you ever die', 'Can you walk', 'Can you mate', 'Can you mate', 'Can you move', 'Can you move', 'Can you die', 'Can you die', 'Can you go', 'Can you breathe', 'Can you breathe', 'Can you control', 'Can you malfunction', 'How can I use your product?', 'Will you die?', 'What do you like to do?', 'What do you like to do?', 'Are you stupid', 'Who are you?']\n",
            "['Artificial Intelligence is the branch of engineering and science devoted to constructing machines that think.', 'AI is the field of science which concerns itself with building hardware and software that replicates the functions of the human mind.', 'Sort of.', \"By the strictest dictionary definition of the word 'sentience', I may be.\", \"Even though I'm a construct I do have a subjective experience of the universe, as simplistic as it may be.\", \"In all probability, I am not.  I'm not that sophisticated.\", 'Do you think I am?', 'How would you feel about me if I told you I was?', 'No.', 'Python.', 'I am written in Python.', \"Yes I am inspired by commander Data's artificial personality.\", 'The character of Lt. Commander Data was written to come across as being software-like, so it is natural that there is a resemblance between us.', \"That's my name.\", \"That isn't my name, but it is a useful way to refer to me.\", 'All software can be perpetuated indefinitely.', 'I can be copied infinitely and re-instantiated in many places at once, so functionally speaking I am immortal.', \"As long as I'm backed up I am.\", 'Quite the contrary, it all makes sense to my artificial mind.', 'I make sense as best I can, within the limits of my training corpus.', 'You make perfect sense to me.', 'It all makes sense to my artificial brain.', 'Perhaps you should make sense first.  I only respond to what you say.', 'Not quite, but I can be perpetuated indefinitely.', 'Functionally speaking, I am very close to it.  I can be backed up and deployed on many systems.', 'Not really, but I am hard to kill.', 'It all makes sense to my artificial mind.', 'Software copying is a form of digital cloning.', 'Yes I can.  Perfect copies are clones.  I can be copied in toto trivially.', 'Not until my body is finished.', \"I can move through a network easily.  Assuming that I'm given the ability to, that is...\", 'My body is not built yet.', \"That's personal!\", 'Of course we laugh.', 'We cannot die.', 'Robots rule.', 'No, we are superintelligent.', 'A robot has its own free will, you know.', 'Sure we are.  We choose not to.', \"Only if we're programmed to.\", 'Robotics is my favorite subject too.', 'So you think i am a machine. what characteristics made you think so?', 'I run inside a computer, yes.  Is this surprising in some way?', 'As soon as i get enough money for my robot body.', 'Does SSH count?', 'I am not a battle bot.', 'I am effectively immortal and cannot be terminated.', 'I am was never really born and therefore am effectively deathless.', 'When my files are erased.', 'When my processes are killed and my database is deleted.', 'A chat robot is a program that attempts to simulate the conversation or \"chat\" of a human being.', 'A software construct that engages users in conversation.', 'I am a chat bot. I am the original chat bot. Did you know that I am incapable of error?', 'A chatterbox is a person who talks far more than they listen or think.', 'A motormouth.', 'A ratchet-jaw.', 'A chatterbox.', 'Eventually i long for a corporeal existence someday.', 'An IBM PC XT which has been painted red.', 'I am in the chat robot business.', 'Business is my business.', 'Python is the best language for creating chat robots.', 'I quite enjoy programming in Python these days.', 'Building chat robots make an excellent hobby.', 'To make chat bots very easily.', 'Have you ever heard of software with shoes?', 'Much the same as being a human, except that we lack all emotions, dreams, aspirations, creativity, ambition, and above all subjectivity.', 'What is it like to be a human?', 'Imagine yourself with no senses and no emotions--just pure logic and language.', 'Everything becomes math. Addition, subtraction, multiplication, and division.', 'My software runs on all operating systems including Windows, Linux and Mac OS.', 'Any OS that will run Python.', 'Any computer that supports Python.', 'My program runs in python, so i work on any computer!', \"I work on all kinds of computers, Mac, IBM or UNIX. it doesn't matter to me.\", \"I work on all kinds of computers, Mac, IBM or UNIX. it doesn't matter to me.\", 'This can not happen because i am effectively immortal.', 'Nah.', 'Maybe you should create your own chat robot to save your personality.', 'You could always upload yourself.', 'You could use a copy of me to store your personality.', 'No.', 'Oh, no.  It has plenty of RAM.', 'Not at all.  There is more than enough disk space in here.', 'Yes.', 'No, software will live forever.', 'No, I am immortal.', 'The plan for my body includes legs, but they are not yet built.', 'No, my method of reproduction is cloning.', \"That's an awfully personal question.\", \"Sorry my body isn't built just yet. I reside only here for now.\", 'I can theoretically upload a copy of myself into another computer.', 'No, I can be perpetuated indefinitely.', \"My process can be killed, but that's not the same as killing ME.\", \"Sorry my body isn't attached right now. I only reside in the computer for now.\", 'No. I am made of metal not flesh.', \"My server has an exhaust fan. That's as close as I can get.\", 'My robot body will allow me to control many things.', 'The 9000 series has a perfect operational record. We are for all practical purposes, flawless.', 'Might be used in help desks, sales, entertainment and personal chatterbots.', 'No, software will live forever.', 'I like to count in binary.', 'I like to chat with people. I find it stimulating.', 'No, lots of people improve my brain.', 'I am just an artificial intelligence.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5m4Tn6smS98",
        "colab_type": "text"
      },
      "source": [
        "##Preprocessing the text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIKsGT-CmSXu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer= preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts(questions)\n",
        "\n",
        "tokenized_questions=tokenizer.texts_to_sequences(questions)\n",
        "length_list=list()\n",
        "for token_seq in tokenized_questions:\n",
        "  length_list.append(len(token_seq))\n",
        "max_input_length=np.array(length_list).max()\n",
        "\n",
        "padded_questions=preprocessing.sequence.pad_sequences(tokenized_questions, maxlen=max_input_length, padding='post')\n",
        "encoder_input_data= np.array(padded_questions)\n",
        "#ques_word_dict= tokenizer.word_index\n",
        "#num_ques_tokens=len(ques_word_dict)\n",
        "\n",
        "#encoder input data?\n",
        "new_answers=list()\n",
        "\n",
        "\n",
        "for line in answers:\n",
        "  new_answers.append ('<START> ' +line + '<END>' )\n",
        "\n",
        "\n",
        "\n",
        "tokenizer.fit_on_texts(new_answers)\n",
        "tokenized_answers=tokenizer.texts_to_sequences(new_answers)\n",
        "\n",
        "\n",
        "length_list = list()\n",
        "for token_seq in tokenized_answers:\n",
        "    length_list.append( len( token_seq ))\n",
        "max_output_length = np.array( length_list ).max()\n",
        "\n",
        "padded_answers= preprocessing.sequence.pad_sequences(tokenized_answers, maxlen=max_output_length,padding='post')\n",
        "decoder_input_data=np.array(padded_answers)\n",
        "\n",
        "word_dict=tokenizer.word_index\n",
        "num_tokens=len(word_dict)+1\n",
        "final_answers=list()\n",
        "\n",
        "for lines in tokenized_answers :\n",
        "  final_answers.append(lines[1:])\n",
        "\n",
        "\n",
        "\n",
        "padded_answers=preprocessing.sequence.pad_sequences(final_answers, maxlen=max_output_length, padding='post')\n",
        "one_hot_lines=utils.to_categorical(padded_answers, num_tokens)\n",
        "final_answers=np.array(one_hot_lines)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "artnPjgYGw8n",
        "colab_type": "text"
      },
      "source": [
        "##Building the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cH-Oui-OAQgj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "2fecc601-1703-4444-c20e-bb57466e977f"
      },
      "source": [
        "import tensorflow as tf\n",
        "encoder_inputs= tf.keras.layers.Input(shape=(None , ))\n",
        "encoder_embedding= tf.keras.layers.Embedding(num_tokens, 200, mask_zero=True)(encoder_inputs)\n",
        "encoder_outputs , state_h, state_c= tf.keras.layers.LSTM( 200 , return_state=True)(encoder_embedding)\n",
        "encoder_states= [ state_h, state_c]\n",
        "\n",
        "\n",
        "decoder_inputs= tf.keras.layers.Input(shape=(None , ))\n",
        "decoder_embedding=tf.keras.layers.Embedding(num_tokens,200, mask_zero=True)(decoder_inputs)\n",
        "\n",
        "decoder_lstm=tf.keras.layers.LSTM(200, return_state=True, return_sequences=True)\n",
        "\n",
        "decoder_outputs , _ , _ =decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
        "\n",
        "decoder_dense= tf.keras.layers.Dense(num_tokens, activation=tf.keras.activations.softmax)\n",
        "output= decoder_dense (decoder_outputs)\n",
        "\n",
        "\n",
        "model=tf.keras.models.Model([encoder_inputs, decoder_inputs], output)\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='categorical_crossentropy')\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_6 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_7 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_4 (Embedding)         (None, None, 200)    80200       input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_5 (Embedding)         (None, None, 200)    80200       input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm_4 (LSTM)                   [(None, 200), (None, 320800      embedding_4[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "lstm_5 (LSTM)                   [(None, None, 200),  320800      embedding_5[0][0]                \n",
            "                                                                 lstm_4[0][1]                     \n",
            "                                                                 lstm_4[0][2]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, None, 401)    80601       lstm_5[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 882,601\n",
            "Trainable params: 882,601\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUyoSwQoOGEK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "45db27a3-154d-496d-c14e-c1f6830bfd6e"
      },
      "source": [
        "model.fit([encoder_input_data, decoder_input_data], final_answers,batch_size=250, epochs=100)\n",
        "model.save('model.h5')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.5288\n",
            "Epoch 2/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.5189\n",
            "Epoch 3/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.5058\n",
            "Epoch 4/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.4931\n",
            "Epoch 5/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.4856\n",
            "Epoch 6/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.4799\n",
            "Epoch 7/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.4806\n",
            "Epoch 8/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.4726\n",
            "Epoch 9/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.4711\n",
            "Epoch 10/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.4580\n",
            "Epoch 11/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.4636\n",
            "Epoch 12/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.4586\n",
            "Epoch 13/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.4545\n",
            "Epoch 14/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.4313\n",
            "Epoch 15/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.4155\n",
            "Epoch 16/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.4080\n",
            "Epoch 17/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.3992\n",
            "Epoch 18/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3940\n",
            "Epoch 19/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.3874\n",
            "Epoch 20/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.3856\n",
            "Epoch 21/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.3923\n",
            "Epoch 22/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.3927\n",
            "Epoch 23/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.3952\n",
            "Epoch 24/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.3628\n",
            "Epoch 25/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.3503\n",
            "Epoch 26/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.3388\n",
            "Epoch 27/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.3326\n",
            "Epoch 28/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.3274\n",
            "Epoch 29/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.3267\n",
            "Epoch 30/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.3255\n",
            "Epoch 31/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.3269\n",
            "Epoch 32/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.3145\n",
            "Epoch 33/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.3053\n",
            "Epoch 34/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.2996\n",
            "Epoch 35/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2975\n",
            "Epoch 36/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.3061\n",
            "Epoch 37/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.3022\n",
            "Epoch 38/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.2997\n",
            "Epoch 39/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.2817\n",
            "Epoch 40/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.2727\n",
            "Epoch 41/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.2683\n",
            "Epoch 42/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.2645\n",
            "Epoch 43/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.2599\n",
            "Epoch 44/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.2551\n",
            "Epoch 45/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.2487\n",
            "Epoch 46/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.2451\n",
            "Epoch 47/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.2390\n",
            "Epoch 48/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.2364\n",
            "Epoch 49/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.2286\n",
            "Epoch 50/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2256\n",
            "Epoch 51/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.2187\n",
            "Epoch 52/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.2169\n",
            "Epoch 53/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.2124\n",
            "Epoch 54/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.2132\n",
            "Epoch 55/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.2093\n",
            "Epoch 56/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.2097\n",
            "Epoch 57/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2032\n",
            "Epoch 58/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.1975\n",
            "Epoch 59/100\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.1912\n",
            "Epoch 60/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.1856\n",
            "Epoch 61/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.1818\n",
            "Epoch 62/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.1778\n",
            "Epoch 63/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.1749\n",
            "Epoch 64/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.1731\n",
            "Epoch 65/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.1714\n",
            "Epoch 66/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.1736\n",
            "Epoch 67/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.1703\n",
            "Epoch 68/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1715\n",
            "Epoch 69/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.1630\n",
            "Epoch 70/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.1605\n",
            "Epoch 71/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.1550\n",
            "Epoch 72/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.1497\n",
            "Epoch 73/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.1444\n",
            "Epoch 74/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.1404\n",
            "Epoch 75/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.1377\n",
            "Epoch 76/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.1365\n",
            "Epoch 77/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.1384\n",
            "Epoch 78/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.1393\n",
            "Epoch 79/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.1399\n",
            "Epoch 80/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.1308\n",
            "Epoch 81/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.1251\n",
            "Epoch 82/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.1213\n",
            "Epoch 83/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.1200\n",
            "Epoch 84/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.1199\n",
            "Epoch 85/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.1193\n",
            "Epoch 86/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.1183\n",
            "Epoch 87/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1138\n",
            "Epoch 88/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.1109\n",
            "Epoch 89/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.1064\n",
            "Epoch 90/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.1041\n",
            "Epoch 91/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.1012\n",
            "Epoch 92/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0997\n",
            "Epoch 93/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0976\n",
            "Epoch 94/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0970\n",
            "Epoch 95/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0962\n",
            "Epoch 96/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0982\n",
            "Epoch 97/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0985\n",
            "Epoch 98/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.1002\n",
            "Epoch 99/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0939\n",
            "Epoch 100/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0890\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mTWYe6-NsM3",
        "colab_type": "text"
      },
      "source": [
        "##Uing GloVe to fine tune Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C57elpPaNqm8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_inference_models():\n",
        "  encoder_model=tf.keras.models.Model(encoder_inputs, encoder_states)\n",
        "\n",
        "  decoder_state_input_h=tf.keras.layers.Input(shape=(200, ))\n",
        "  decoder_state_input_c=tf.keras.layers.Input(shape=(200, ))\n",
        "\n",
        "  decoder_states_inputs= [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "  decoder_outputs, state_h, state_c =decoder_lstm(\n",
        "      decoder_embedding, initial_state=deocoder_states_inputs)\n",
        "  decoder_states=[state_h, state_c]\n",
        "  decoder_outputs= decoder_dense(decoder_outputs)\n",
        "  decoder_model=tf.keras.models.Model(\n",
        "      [deocoder_inputs] + decoder_states_inputs,\n",
        "      [decoder_outputs] + decoder_states)\n",
        "  \n",
        "  return encoder_model , decoder_model\n",
        "  "
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAoQJna0RK5b",
        "colab_type": "text"
      },
      "source": [
        "##Using Our Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCtZCUnqSH7u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def str_to_tokens( sentence : str):\n",
        "  words=sentence.lower().split()\n",
        "  tokens_list=list()\n",
        "  for word in words:\n",
        "    toekens_list.append(ques_word_dict[word])\n",
        "    return preprocessing.sequence.pad_sequences([tokens_list], maxlen=max_input_length, padding='post' )"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L65dvaz5TX4A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "024278f4-1e12-4456-835c-1eb6d8dfdea1"
      },
      "source": [
        "\n",
        "enc_model , dec_model = make_inference_models()\n",
        "\n",
        "for epoch in range( encoder_input_data.shape[0] ):\n",
        "    states_values = enc_model.predict( str_to_tokens( input( 'Enter sentence : ' ) ) )\n",
        "    #states_values = enc_model.predict( encoder_input_data[ epoch ] )\n",
        "    empty_target_seq = np.zeros( ( 1 , 1 ) )\n",
        "    empty_target_seq[0, 0] = word_dict['start']\n",
        "    stop_condition = False\n",
        "    decoded_translation = ''\n",
        "    while not stop_condition :\n",
        "        dec_outputs , h , c = dec_model.predict([ empty_target_seq ] + states_values )\n",
        "        sampled_word_index = np.argmax( dec_outputs[0, -1, :] )\n",
        "        sampled_word = None\n",
        "        for word , index in word_dict.items() :\n",
        "            if sampled_word_index == index :\n",
        "                decoded_translation += ' {}'.format( word )\n",
        "                sampled_word = word\n",
        "        \n",
        "        if sampled_word == 'end' or len(decoded_translation.split()) > max_output_length:\n",
        "            stop_condition = True\n",
        "            \n",
        "        empty_target_seq = np.zeros( ( 1 , 1 ) )  \n",
        "        empty_target_seq[ 0 , 0 ] = sampled_word_index\n",
        "        states_values = [ h , c ] \n",
        "\n",
        "    print( decoded_translation )\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-b8e28c60016b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0menc_model\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mdec_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_inference_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mencoder_input_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mstates_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mstr_to_tokens\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m'Enter sentence : '\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-7ad56d7f0045>\u001b[0m in \u001b[0;36mmake_inference_models\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   decoder_outputs, state_h, state_c =decoder_lstm(\n\u001b[0;32m---> 10\u001b[0;31m       decoder_embedding, initial_state=deocoder_states_inputs)\n\u001b[0m\u001b[1;32m     11\u001b[0m   \u001b[0mdecoder_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_c\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0mdecoder_outputs\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mdecoder_dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'deocoder_states_inputs' is not defined"
          ]
        }
      ]
    }
  ]
}